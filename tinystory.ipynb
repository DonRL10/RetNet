{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from model import RetNET\n",
    "from tinystories import Task\n",
    "\n",
    "from functools import partial"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "iters = partial(Task.iter_batches, batch_size = 32, max_seq_len = 256, device = 'cuda', num_workers = 4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def estimate_loss():\n",
    "    out = {}\n",
    "    model.eval()\n",
    "    for split in ['train', 'val']:\n",
    "        batch_iter = iters(split)\n",
    "        losses = torch.zeros(eval_iters)\n",
    "        for k in range(eval_iters):\n",
    "            X, Y = next(batch_iter)\n",
    "            _, loss, _ = model(X, Y)\n",
    "            losses[k] = loss.item()\n",
    "        out[split] = losses.mean()\n",
    "    model.train()\n",
    "    return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ModelConifg:\n",
    "    nembed = 288\n",
    "    nheads = 6\n",
    "    nlayers = 6\n",
    "\n",
    "    vocab_size = 32000\n",
    "    block_size = 256\n",
    "\n",
    "    device = 'cuda'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = ModelConifg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parametres 22951904\n"
     ]
    }
   ],
   "source": [
    "model = RetNET(config).to('cuda')\n",
    "print(\"Parametres\", sum(p.numel() for p in model.parameters()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/5000 [00:09<?, ?it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[7], line 23\u001b[0m\n\u001b[0;32m     21\u001b[0m \u001b[39m# pbar.set_description(f\"Loss {loss.item():.4f}\")\u001b[39;00m\n\u001b[0;32m     22\u001b[0m \u001b[39mif\u001b[39;00m i \u001b[39m%\u001b[39m \u001b[39m500\u001b[39m\u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[1;32m---> 23\u001b[0m     losses \u001b[39m=\u001b[39m estimate_loss()\n\u001b[0;32m     24\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39m\\n\u001b[39;00m\u001b[39mStep \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m: train_loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m  val_loss \u001b[39m\u001b[39m{\u001b[39;00mlosses[\u001b[39m'\u001b[39m\u001b[39mval\u001b[39m\u001b[39m'\u001b[39m]\u001b[39m:\u001b[39;00m\u001b[39m.4f\u001b[39m\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\utils\\_contextlib.py:115\u001b[0m, in \u001b[0;36mcontext_decorator.<locals>.decorate_context\u001b[1;34m(*args, **kwargs)\u001b[0m\n\u001b[0;32m    112\u001b[0m \u001b[39m@functools\u001b[39m\u001b[39m.\u001b[39mwraps(func)\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mdecorate_context\u001b[39m(\u001b[39m*\u001b[39margs, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs):\n\u001b[0;32m    114\u001b[0m     \u001b[39mwith\u001b[39;00m ctx_factory():\n\u001b[1;32m--> 115\u001b[0m         \u001b[39mreturn\u001b[39;00m func(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n",
      "Cell \u001b[1;32mIn[3], line 10\u001b[0m, in \u001b[0;36mestimate_loss\u001b[1;34m()\u001b[0m\n\u001b[0;32m      8\u001b[0m \u001b[39mfor\u001b[39;00m k \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(eval_iters):\n\u001b[0;32m      9\u001b[0m     X, Y \u001b[39m=\u001b[39m \u001b[39mnext\u001b[39m(batch_iter)\n\u001b[1;32m---> 10\u001b[0m     _, loss, _ \u001b[39m=\u001b[39m model(X, Y)\n\u001b[0;32m     11\u001b[0m     losses[k] \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mitem()\n\u001b[0;32m     12\u001b[0m out[split] \u001b[39m=\u001b[39m losses\u001b[39m.\u001b[39mmean()\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rohan\\Desktop\\learning\\REtNET\\model.py:106\u001b[0m, in \u001b[0;36mRetNET.forward\u001b[1;34m(self, idx, targets, past_kv, type, offset)\u001b[0m\n\u001b[0;32m    104\u001b[0m \u001b[39mfor\u001b[39;00m i, layer \u001b[39min\u001b[39;00m \u001b[39menumerate\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mblocks):\n\u001b[0;32m    105\u001b[0m     past_kv_i \u001b[39m=\u001b[39m past_kv[i] \u001b[39mif\u001b[39;00m past_kv \u001b[39m!=\u001b[39m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m--> 106\u001b[0m     x, kv_i \u001b[39m=\u001b[39m layer(x, past_kv_i, \u001b[39mtype\u001b[39;49m \u001b[39m=\u001b[39;49m \u001b[39mtype\u001b[39;49m, offset \u001b[39m=\u001b[39;49m offset)\n\u001b[0;32m    107\u001b[0m     kv_cache\u001b[39m.\u001b[39mappend(kv_i)\n\u001b[0;32m    109\u001b[0m \u001b[39m# x = self.rms(x)\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rohan\\Desktop\\learning\\REtNET\\model.py:70\u001b[0m, in \u001b[0;36mFused.forward\u001b[1;34m(self, x, past_kv, type, offset)\u001b[0m\n\u001b[0;32m     67\u001b[0m q, k, v, ff \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mqkvff(x_)\u001b[39m.\u001b[39msplit(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mfused_dims, dim \u001b[39m=\u001b[39m \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\n\u001b[0;32m     69\u001b[0m q \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mxpos(q, offset \u001b[39m=\u001b[39m offset)\n\u001b[1;32m---> 70\u001b[0m k \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mxpos(k, offset \u001b[39m=\u001b[39;49m offset, downscale \u001b[39m=\u001b[39;49m \u001b[39mTrue\u001b[39;49;00m)\n\u001b[0;32m     72\u001b[0m q \u001b[39m=\u001b[39m q\u001b[39m.\u001b[39mview(B, T, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnheads, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n\u001b[0;32m     73\u001b[0m v \u001b[39m=\u001b[39m v\u001b[39m.\u001b[39mview(B, T, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mnheads, \u001b[39m-\u001b[39m\u001b[39m1\u001b[39m)\u001b[39m.\u001b[39mtranspose(\u001b[39m1\u001b[39m, \u001b[39m2\u001b[39m)\n",
      "File \u001b[1;32mc:\\Users\\rohan\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\torch\\nn\\modules\\module.py:1501\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *args, **kwargs)\u001b[0m\n\u001b[0;32m   1496\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1497\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1498\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_pre_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks\n\u001b[0;32m   1499\u001b[0m         \u001b[39mor\u001b[39;00m _global_backward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1500\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1501\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39;49margs, \u001b[39m*\u001b[39;49m\u001b[39m*\u001b[39;49mkwargs)\n\u001b[0;32m   1502\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1503\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
      "File \u001b[1;32mc:\\Users\\rohan\\Desktop\\learning\\REtNET\\xpos.py:64\u001b[0m, in \u001b[0;36mXPOS.forward\u001b[1;34m(self, x, offset, downscale)\u001b[0m\n\u001b[0;32m     61\u001b[0m \u001b[39mif\u001b[39;00m downscale:\n\u001b[0;32m     62\u001b[0m     scale \u001b[39m=\u001b[39m \u001b[39m1\u001b[39m \u001b[39m/\u001b[39m scale\n\u001b[1;32m---> 64\u001b[0m x \u001b[39m=\u001b[39m apply_rotary_pos_emb(x, sin, cos, scale)\n\u001b[0;32m     65\u001b[0m \u001b[39mreturn\u001b[39;00m x\n",
      "File \u001b[1;32mc:\\Users\\rohan\\Desktop\\learning\\REtNET\\xpos.py:35\u001b[0m, in \u001b[0;36mapply_rotary_pos_emb\u001b[1;34m(x, sin, cos, scale)\u001b[0m\n\u001b[0;32m     33\u001b[0m sin, cos \u001b[39m=\u001b[39m \u001b[39mmap\u001b[39m(\u001b[39mlambda\u001b[39;00m t: duplicate_interleave(t \u001b[39m*\u001b[39m scale), (sin, cos))\n\u001b[0;32m     34\u001b[0m \u001b[39m# einsum notation for lambda t: repeat(t[offset:x.shape[1]+offset,:], \"n d -> () n () (d j)\", j=2)\u001b[39;00m\n\u001b[1;32m---> 35\u001b[0m \u001b[39mreturn\u001b[39;00m (x \u001b[39m*\u001b[39m cos) \u001b[39m+\u001b[39m (rotate_every_two(x) \u001b[39m*\u001b[39m sin)\n",
      "File \u001b[1;32mc:\\Users\\rohan\\Desktop\\learning\\REtNET\\xpos.py:19\u001b[0m, in \u001b[0;36mrotate_every_two\u001b[1;34m(x)\u001b[0m\n\u001b[0;32m     17\u001b[0m x1 \u001b[39m=\u001b[39m x[:, :, ::\u001b[39m2\u001b[39m]\n\u001b[0;32m     18\u001b[0m x2 \u001b[39m=\u001b[39m x[:, :, \u001b[39m1\u001b[39m::\u001b[39m2\u001b[39m]\n\u001b[1;32m---> 19\u001b[0m x \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39;49mstack((\u001b[39m-\u001b[39;49mx2, x1), dim\u001b[39m=\u001b[39;49m\u001b[39m-\u001b[39;49m\u001b[39m1\u001b[39;49m)\n\u001b[0;32m     20\u001b[0m \u001b[39mreturn\u001b[39;00m x\u001b[39m.\u001b[39mflatten(\u001b[39m-\u001b[39m\u001b[39m2\u001b[39m)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_iters = 5000\n",
    "eval_iters = 50\n",
    "optimizer = torch.optim.AdamW(model.parameters(), lr = 3e-4)\n",
    "\n",
    "train_iters = iters('train')\n",
    "xb, yb = next(train_iters)\n",
    "\n",
    "from tqdm import tqdm\n",
    "pbar = tqdm(range(max_iters))\n",
    "\n",
    "micro_batches = 2\n",
    "\n",
    "for i in pbar:\n",
    "    for m in range(micro_batches):\n",
    "        _, loss, _ = model(xb, yb)\n",
    "        loss = loss / micro_batches\n",
    "        xb, yb = next(train_iters) # to retrieve batches asyncally on cpu while model in doing its ting in gpu\n",
    "        loss.backward()\n",
    "    optimizer.step()\n",
    "    optimizer.zero_grad(set_to_none = True)\n",
    "    # pbar.set_description(f\"Loss {loss.item():.4f}\")\n",
    "    if i % 500== 0:\n",
    "        losses = estimate_loss()\n",
    "        print(f\"\\nStep {i}: train_loss {losses['train']:.4f}  val_loss {losses['val']:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<All keys matched successfully>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.load_state_dict(torch.load(\"tiny_stories_28866.pt\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tokenizer import Tokeniser"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "enc = Tokeniser()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kyle was very excited to keep everyone safe. One day they went for a drive in their car. As they drove by, they found a beautiful park. In the park was a slide, windows with a rainbow slide. They all laughed and played there too.\n",
      "As the day went on, Kat and their moms let them go in. Everyone was so happy and excited!\n",
      "Felix Celia and their mum would always be ready for all of the animals. They raced around through the park so they could see even small birds in the sky while they drove away.\n",
      "They packed up their carrots and then they returned to their destination.\n",
      "When they arrived, everyone was so excited. They were so excited. They explored the park and finally arrived in the park. It was a beautiful place in the park.\n",
      "\"Look at this expensive thing!\" Rachel said proudly.\n",
      "Welia smiled and they enjoyed the cool trash of their adventure. Billy and her mommy were playing in the park together. Billy was having so much fun, jumping around and swinging in the trees. Suddenly, Billy's mom came in and stopped their running. She said, \"Stop Go away, Billy! You don't want to tie so many to your runs\".\n",
      "Billy was sad, but he understood. He said, \"Okay, okay, okay. I will be more next time!\"\n",
      "B\n"
     ]
    }
   ],
   "source": [
    "context = \"Kyle\"\n",
    "out = model.generate(torch.tensor([enc.encode(context, True, False)], dtype = torch.long, device = 'cuda'), steps = 300)\n",
    "print(\"\".join(enc.decode(out[0].tolist())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.3"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
